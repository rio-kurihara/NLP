{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "サンプルデータは下記Wikipediaより  \n",
    "https://ja.wikipedia.org/wiki/YouTuber "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "from utils.preprocess import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './test.mm'\n",
    "\n",
    "list_docs = [\n",
    "    '2007年5月頃に、YouTubeが特に閲覧数の多いユーザーに「パートナー」（YouTubeパートナープログラム）となるよう勧誘したことが始まりとされる',\n",
    "    'プログラムは当初、商業コンテンツ供給者だけに勧められたものだった）[要出典]。',\n",
    "    '後にビデオ画面の隣側に広告掲載するのを条件にユーザーが利益を得ることが可能になり、2011年4月にはパートナープログラムが一般ユーザーにも開放。',\n",
    "    'より多くのユーザが広告収入を得られるようになった。',\n",
    "    'その後は、YouTubeで収益を得て生活する人物も現れた[7]。',\n",
    "    'なお、2017年4月以降は総再生回数が10,000回以上ではないとパートナープログラムへの参加が認められなくなったが、2018年2月からはさらに過去12か月間の総再生時間が4,000時間以上、かつチャンネル登録者数が1,000人以上とより厳しい条件に引き上げられた',\n",
    "    '加えてコミュニティのストライキ、スパムなどの監視が強化され、ポリシーに準拠していることの評価が厳格化されるようになった。',\n",
    "    'マスメディアの露出がほとんどないにもかかわらず、年収が数億〜十数億円にも達する者・独自に握手会等のイベントを開く者もいるが、ごく少数である。'\n",
    "]\n",
    "indices = [i for i, _ in enumerate(range(0, len(list_docs)))]\n",
    "\n",
    "path_dict = '/usr/lib/mecab/dic/mecab-ipadic-neologd/'\n",
    "tkn = tokenize.Tokenize(path_dict=path_dict)\n",
    "list_split = [tkn.tokenize_text(x) for x in list_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tpc_0: 0.014*\"YouTube\" + 0.012*\"者\" + 0.012*\"]。\" + 0.012*\"も\" + 0.011*\"億\" + 0.010*\"だ\" + 0...\n",
      "tpc_1: 0.014*\"得る\" + 0.014*\"多く\" + 0.014*\"広告収入\" + 0.014*\"ユーザ\" + 0.013*\"て\" + 0.011*\"よう\" + ...\n",
      "tpc_2: 0.014*\"000\" + 0.014*\",\" + 0.014*\"以上\" + 0.011*\"総\" + 0.011*\"時間\" + 0.010*\"ない\" + 0.0...\n"
     ]
    }
   ],
   "source": [
    "# LDA\n",
    "dictionary = corpora.Dictionary(list_split)\n",
    "corpus = [dictionary.doc2bow(list_word) for list_word in list_split]\n",
    "corpora.MmCorpus.serialize(save_path, corpus) # saving\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "num_topics = 3\n",
    "lda = models.LdaModel(corpus=corpus_tfidf, id2word=dictionary,\n",
    "                             num_topics=num_topics, minimum_probability=0.001,\n",
    "                             passes=20, update_every=0, chunksize=10000)\n",
    "\n",
    "for i in range(num_topics):\n",
    "    print('tpc_{0}: {1}'.format(i, lda.print_topic(i)[0:80]+'...'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
